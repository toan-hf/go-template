{{$asg_config := yaml "./workers.yaml" -}}

# DO NOT EDIT THIS FILE DIRECTLY
## EDIT ./workers.yaml 
## RUN make workers.tf 

{{ range $workers_type, $workers_config := $asg_config.workers}}
data "template_file" "{{ $workers_type }}_workers_user_data" {
  template = "${file("./templates/workers_user_data.tpl.sh")}"

  vars {
    aws_region   = "${var.aws_region}"
    cluster_name = "${module.cluster_label.id}"

    node_labels      = "lifecycle=OnDemand,{{ join "," $workers_config.labels.onDemand }}"
    node_taints      = "{{ join "," $workers_config.taints}}"
    spot_node_labels = "lifecycle=Ec2Spot,{{ join "," $workers_config.labels.spot }}"
    {{- set $ "spot_node_taints" (prepend $workers_config.taints "spotInstance=true:PreferNoSchedule") }}
    spot_node_taints = "{{ join "," $.spot_node_taints }}"

    {{- with $workers_config.evictionHard }}
    eviction_hard = "{{ . }}"
    {{- end }}
    
    {{- with $workers_config.kubeReserved }}
    kube_reserved = "{{ . }}"
    {{- end }}
    
    {{- with $workers_config.systemReserved }}
    system_reserved = "{{ . }}"
    {{- end }}
  }
}

resource "aws_launch_template" "{{ $workers_type }}_workers" {
  name                   = "${module.cluster_label.id}-{{ $workers_type }}-workers"
  image_id               = "${data.aws_ami.worker.id}"
  instance_type          = "{{ index $workers_config.instanceTypes 0 }}"

  credit_specification {
    # T3 instances are launched as unlimited by default. T2 instances are launched as standard by default.
    cpu_credits = "standard"  # Can be "standard" or "unlimited"
  }

  network_interfaces {
    security_groups = {{ toHcl $workers_config.securityGroups }}
    device_index = 0
    associate_public_ip_address = false
  }

  iam_instance_profile   {
    name = "${aws_iam_instance_profile.{{$workers_config.iamInstanceProfile}}.name}"
  }

  tag_specifications {
    resource_type = "instance"
    tags = "${merge(module.cluster_label.tags,map(
      "Name","${module.cluster_label.id}-{{ $workers_type }}-worker-ASG-Node",
      "KubernetesCluster","${module.cluster_label.id}",
      "kubernetes.io/cluster/${module.cluster_label.id}","owned"
    ))}"
  }

  user_data              = "${base64encode(data.template_file.{{ $workers_type }}_workers_user_data.rendered)}"
  key_name               = "${ {{ $asg_config.common.key_name }} }"

  tags = "${module.cluster_label.tags}"
}

resource "aws_autoscaling_group" "{{ $workers_type }}_workers" {
  name                = "${module.cluster_label.id}-{{ $workers_type }}-workers"
  vpc_zone_identifier = ["${values({{ $asg_config.common.subnets }})}"]
  min_size            = {{ $workers_config.asg.minSize }}
  desired_capacity    = {{ $workers_config.asg.desiredCapacity }}
  max_size            = {{ $workers_config.asg.maxSize }}

  mixed_instances_policy {
    instances_distribution {
      on_demand_allocation_strategy            = "prioritized" # Valid values: prioritized. Default: prioritized
      spot_allocation_strategy                 = "lowest-price" # Valid values: lowest-price. Default: lowest-price.
      on_demand_base_capacity                  = {{ $workers_config.asg.onDemandBaseCapacity }}
      on_demand_percentage_above_base_capacity = {{ $workers_config.asg.onDemandPercentageAboveBaseCapacity }}
      # EC2 Auto Scaling selects the cheapest Spot pools and evenly allocates Spot capacity across the number of Spot pools that you specify.
      spot_instance_pools                      = {{ $workers_config.asg.spotInstancePools }} # Default: 2
    }

    launch_template {
      launch_template_specification {
        launch_template_id = "${aws_launch_template.{{ $workers_type }}_workers.id}"
        version            = "$$Latest"
      }
      {{- range $instanceType := $workers_config.instanceTypes }}
      override {
        instance_type = "{{ $instanceType }}"
      }
      {{- end}}
    }
  }

  tags = [
    {
      key                         = "Namespace"
      value                       = "${var.namespace}"
      propagate_at_launch         = true
    },
    {
      key                         = "Stage"
      value                       = "${var.stage}"
      propagate_at_launch         = true
    },
    {
      key                        = "Name"
      value                      = "${module.cluster_label.id}-{{ $workers_type }}-workers-ASG-Node"
      propagate_at_launch        = true
    },
    {
      key                        = "kubernetes.io/cluster/${module.cluster_label.id}"
      value                      = "owned"
      propagate_at_launch        = true
    },
    {{- if $workers_config.autoscaling }}
    {
      key                        = "k8s.io/cluster-autoscaler/enabled"
      value                      = "true"
      propagate_at_launch        = true
    },
    {{- end }}
  ]

  # Allowed values are Launch, Terminate, HealthCheck, ReplaceUnhealthy, 
  # AZRebalance, AlarmNotification, ScheduledActions, AddToLoadBalancer.
  suspended_processes = [
    "AZRebalance",
  ]

  depends_on = {{ toHcl $workers_config.tfDependencies }}

  {{- if $workers_config.autoscaling }}
  lifecycle {
    ignore_changes = [ "desired_capacity" ]
  }
  {{- end}}
}

resource "aws_autoscaling_lifecycle_hook" "{{ $workers_type }}_workers" {
  name                   = "${module.cluster_label.id}-{{ $workers_type }}-workers-nodedrainerLCH"
  autoscaling_group_name = "${aws_autoscaling_group.{{ $workers_type }}_workers.name}"
  default_result         = "CONTINUE"
  heartbeat_timeout      = 300
  lifecycle_transition   = "autoscaling:EC2_INSTANCE_TERMINATING"

  notification_target_arn = "${aws_sqs_queue.instance_termination.arn}"
  role_arn                = "${aws_iam_role.autoscaling_instance_termination_notifications.arn}"
}
{{- end }}
