# DO NOT EDIT THIS FILE DIRECTLY
## EDIT ./workers.yaml 
## RUN make workers.tf 


data "template_file" "compute_workers_user_data" {
  template = file("./templates/workers_user_data.tpl.sh")

  vars {
    aws_region   = var.aws_region
    cluster_name = module.cluster_label.id

    node_labels      = "lifecycle=OnDemand,node-role.kubernetes.io/worker=true"
    node_taints      = ""
    spot_node_labels = "lifecycle=Ec2Spot,node-role.kubernetes.io/spot-worker=true"
    spot_node_taints = "spotInstance=true:PreferNoSchedule"
    eviction_hard    = "memory.available<750Mi,nodefs.available<10%,nodefs.inodesFree<5%,imagefs.available<10%,imagefs.inodesFree<5%"
    kube_reserved    = "cpu=250m,memory=1Gi,ephemeral-storage=1Gi"
    system_reserved  = "cpu=250m,memory=0.2Gi,ephemeral-storage=1Gi"
  }
}

resource "aws_launch_template" "compute_workers" {
  name          = "${module.cluster_label.id}-compute-workers"
  image_id      = data.aws_ami.worker.id
  instance_type = "m5ad.2xlarge"

  credit_specification {
    # T3 instances are launched as unlimited by default. T2 instances are launched as standard by default.
    cpu_credits = "standard" # Can be "standard" or "unlimited"
  }

  network_interfaces {
    security_groups             = ["${aws_security_group.workers.id}"]
    device_index                = 0
    associate_public_ip_address = false
  }

  iam_instance_profile {
    name = aws_iam_instance_profile.workers.name
  }

  tag_specifications {
    resource_type = "instance"
    tags = (merge(module.cluster_label.tags, map(
      "Name", "${module.cluster_label.id}-compute-worker-ASG-Node",
      "KubernetesCluster", "${module.cluster_label.id}",
      "kubernetes.io/cluster/${module.cluster_label.id}", "owned"
    )))
  }

  user_data = base64encode(data.template_file.compute_workers_user_data.rendered)
  key_name  = data.terraform_remote_state.staging.ec2_key_name

  tags = module.cluster_label.tags
}

resource "aws_autoscaling_group" "compute_workers" {
  name                = "${module.cluster_label.id}-compute-workers"
  vpc_zone_identifier = ["${values(data.terraform_remote_state.staging.private_subnets_bohr)}"]
  min_size            = 1
  desired_capacity    = 6
  max_size            = 18

  mixed_instances_policy {
    instances_distribution {
      on_demand_allocation_strategy            = "prioritized"  # Valid values: prioritized. Default: prioritized
      spot_allocation_strategy                 = "lowest-price" # Valid values: lowest-price. Default: lowest-price.
      on_demand_base_capacity                  = 1
      on_demand_percentage_above_base_capacity = 0
      # EC2 Auto Scaling selects the cheapest Spot pools and evenly allocates Spot capacity across the number of Spot pools that you specify.
      spot_instance_pools = 2 # Default: 2
    }

    launch_template {
      launch_template_specification {
        launch_template_id = aws_launch_template.compute_workers.id
        version            = "$$Latest"
      }
      override {
        instance_type = "m5ad.2xlarge"
      }
      override {
        instance_type = "c5d.2xlarge"
      }
    }
  }

  tags = [
    {
      key                 = "Namespace"
      value               = "${var.namespace}"
      propagate_at_launch = true
    },
    {
      key                 = "Stage"
      value               = "${var.stage}"
      propagate_at_launch = true
    },
    {
      key                 = "Name"
      value               = "${module.cluster_label.id}-compute-workers-ASG-Node"
      propagate_at_launch = true
    },
    {
      key                 = "kubernetes.io/cluster/${module.cluster_label.id}"
      value               = "owned"
      propagate_at_launch = true
    },
    {
      key                 = "k8s.io/cluster-autoscaler/enabled"
      value               = "true"
      propagate_at_launch = true
    },
  ]

  # Allowed values are Launch, Terminate, HealthCheck, ReplaceUnhealthy, 
  # AZRebalance, AlarmNotification, ScheduledActions, AddToLoadBalancer.
  suspended_processes = [
    "AZRebalance",
  ]

  depends_on = ["aws_eks_cluster.main", "aws_iam_role_policy_attachment.workers_EKSWorkerNodePolicy", "aws_iam_role_policy_attachment.workers_EKS_CNI_Policy"]
  lifecycle {
    ignore_changes = ["desired_capacity"]
  }
}

resource "aws_autoscaling_lifecycle_hook" "compute_workers" {
  name                   = "${module.cluster_label.id}-compute-workers-nodedrainerLCH"
  autoscaling_group_name = aws_autoscaling_group.compute_workers.name
  default_result         = "CONTINUE"
  heartbeat_timeout      = 300
  lifecycle_transition   = "autoscaling:EC2_INSTANCE_TERMINATING"

  notification_target_arn = aws_sqs_queue.instance_termination.arn
  role_arn                = aws_iam_role.autoscaling_instance_termination_notifications.arn
}
data "template_file" "data_workers_user_data" {
  template = file("./templates/workers_user_data.tpl.sh")

  vars {
    aws_region   = var.aws_region
    cluster_name = module.cluster_label.id

    node_labels      = "lifecycle=OnDemand,node-role.kubernetes.io/data-worker=true"
    node_taints      = "data-worker=true:NoSchedule"
    spot_node_labels = "lifecycle=Ec2Spot,node-role.kubernetes.io/spot-data=true"
    spot_node_taints = "spotInstance=true:PreferNoSchedule,data-worker=true:NoSchedule"
    eviction_hard    = "memory.available<750Mi,nodefs.available<10%,nodefs.inodesFree<5%,imagefs.available<10%,imagefs.inodesFree<5%"
    kube_reserved    = "cpu=250m,memory=1Gi,ephemeral-storage=1Gi"
    system_reserved  = "cpu=250m,memory=0.2Gi,ephemeral-storage=1Gi"
  }
}

resource "aws_launch_template" "data_workers" {
  name          = "${module.cluster_label.id}-data-workers"
  image_id      = data.aws_ami.worker.id
  instance_type = "m5a.2xlarge"

  credit_specification {
    # T3 instances are launched as unlimited by default. T2 instances are launched as standard by default.
    cpu_credits = "standard" # Can be "standard" or "unlimited"
  }

  network_interfaces {
    security_groups             = ["${aws_security_group.workers.id}"]
    device_index                = 0
    associate_public_ip_address = false
  }

  iam_instance_profile {
    name = aws_iam_instance_profile.workers.name
  }

  tag_specifications {
    resource_type = "instance"
    tags = (merge(module.cluster_label.tags, map(
      "Name", "${module.cluster_label.id}-data-worker-ASG-Node",
      "KubernetesCluster", "${module.cluster_label.id}",
      "kubernetes.io/cluster/${module.cluster_label.id}", "owned"
    )))
  }

  user_data = base64encode(data.template_file.data_workers_user_data.rendered)
  key_name  = data.terraform_remote_state.staging.ec2_key_name

  tags = module.cluster_label.tags
}

resource "aws_autoscaling_group" "data_workers" {
  name                = "${module.cluster_label.id}-data-workers"
  vpc_zone_identifier = ["${values(data.terraform_remote_state.staging.private_subnets_bohr)}"]
  min_size            = 1
  desired_capacity    = 2
  max_size            = 3

  mixed_instances_policy {
    instances_distribution {
      on_demand_allocation_strategy            = "prioritized"  # Valid values: prioritized. Default: prioritized
      spot_allocation_strategy                 = "lowest-price" # Valid values: lowest-price. Default: lowest-price.
      on_demand_base_capacity                  = 2
      on_demand_percentage_above_base_capacity = 0
      # EC2 Auto Scaling selects the cheapest Spot pools and evenly allocates Spot capacity across the number of Spot pools that you specify.
      spot_instance_pools = 0 # Default: 2
    }

    launch_template {
      launch_template_specification {
        launch_template_id = aws_launch_template.data_workers.id
        version            = "$$Latest"
      }
      override {
        instance_type = "m5a.2xlarge"
      }
    }
  }

  tags = [
    {
      key                 = "Namespace"
      value               = "${var.namespace}"
      propagate_at_launch = true
    },
    {
      key                 = "Stage"
      value               = "${var.stage}"
      propagate_at_launch = true
    },
    {
      key                 = "Name"
      value               = "${module.cluster_label.id}-data-workers-ASG-Node"
      propagate_at_launch = true
    },
    {
      key                 = "kubernetes.io/cluster/${module.cluster_label.id}"
      value               = "owned"
      propagate_at_launch = true
    },
    {
      key                 = "k8s.io/cluster-autoscaler/enabled"
      value               = "true"
      propagate_at_launch = true
    },
  ]

  # Allowed values are Launch, Terminate, HealthCheck, ReplaceUnhealthy, 
  # AZRebalance, AlarmNotification, ScheduledActions, AddToLoadBalancer.
  suspended_processes = [
    "AZRebalance",
  ]

  depends_on = ["aws_eks_cluster.main", "aws_iam_role_policy_attachment.workers_EKSWorkerNodePolicy", "aws_iam_role_policy_attachment.workers_EKS_CNI_Policy"]
  lifecycle {
    ignore_changes = ["desired_capacity"]
  }
}

resource "aws_autoscaling_lifecycle_hook" "data_workers" {
  name                   = "${module.cluster_label.id}-data-workers-nodedrainerLCH"
  autoscaling_group_name = aws_autoscaling_group.data_workers.name
  default_result         = "CONTINUE"
  heartbeat_timeout      = 300
  lifecycle_transition   = "autoscaling:EC2_INSTANCE_TERMINATING"

  notification_target_arn = aws_sqs_queue.instance_termination.arn
  role_arn                = aws_iam_role.autoscaling_instance_termination_notifications.arn
}
data "template_file" "edge_workers_user_data" {
  template = file("./templates/workers_user_data.tpl.sh")

  vars {
    aws_region   = var.aws_region
    cluster_name = module.cluster_label.id

    node_labels      = "lifecycle=OnDemand,node-role.kubernetes.io/edge=true"
    node_taints      = "edge=true:NoSchedule"
    spot_node_labels = "lifecycle=Ec2Spot,node-role.kubernetes.io/spot-edge=true"
    spot_node_taints = "spotInstance=true:PreferNoSchedule,edge=true:NoSchedule"
    eviction_hard    = "memory.available<100Mi,nodefs.available<10%,nodefs.inodesFree<5%,imagefs.available<10%,imagefs.inodesFree<5%"
    kube_reserved    = "cpu=250m,memory=150Mi,ephemeral-storage=1Gi"
    system_reserved  = "cpu=250m,memory=150Mi,ephemeral-storage=1Gi"
  }
}

resource "aws_launch_template" "edge_workers" {
  name          = "${module.cluster_label.id}-edge-workers"
  image_id      = data.aws_ami.worker.id
  instance_type = "t3.small"

  credit_specification {
    # T3 instances are launched as unlimited by default. T2 instances are launched as standard by default.
    cpu_credits = "standard" # Can be "standard" or "unlimited"
  }

  network_interfaces {
    security_groups             = ["${aws_security_group.edge.id}"]
    device_index                = 0
    associate_public_ip_address = false
  }

  iam_instance_profile {
    name = aws_iam_instance_profile.workers.name
  }

  tag_specifications {
    resource_type = "instance"
    tags = (merge(module.cluster_label.tags, map(
      "Name", "${module.cluster_label.id}-edge-worker-ASG-Node",
      "KubernetesCluster", "${module.cluster_label.id}",
      "kubernetes.io/cluster/${module.cluster_label.id}", "owned"
    )))
  }

  user_data = base64encode(data.template_file.edge_workers_user_data.rendered)
  key_name  = data.terraform_remote_state.staging.ec2_key_name

  tags = module.cluster_label.tags
}

resource "aws_autoscaling_group" "edge_workers" {
  name                = "${module.cluster_label.id}-edge-workers"
  vpc_zone_identifier = ["${values(data.terraform_remote_state.staging.private_subnets_bohr)}"]
  min_size            = 2
  desired_capacity    = 2
  max_size            = 3

  mixed_instances_policy {
    instances_distribution {
      on_demand_allocation_strategy            = "prioritized"  # Valid values: prioritized. Default: prioritized
      spot_allocation_strategy                 = "lowest-price" # Valid values: lowest-price. Default: lowest-price.
      on_demand_base_capacity                  = 1
      on_demand_percentage_above_base_capacity = 0
      # EC2 Auto Scaling selects the cheapest Spot pools and evenly allocates Spot capacity across the number of Spot pools that you specify.
      spot_instance_pools = 2 # Default: 2
    }

    launch_template {
      launch_template_specification {
        launch_template_id = aws_launch_template.edge_workers.id
        version            = "$$Latest"
      }
      override {
        instance_type = "t3.small"
      }
      override {
        instance_type = "t2.small"
      }
      override {
        instance_type = "t3.medium"
      }
    }
  }

  tags = [
    {
      key                 = "Namespace"
      value               = "${var.namespace}"
      propagate_at_launch = true
    },
    {
      key                 = "Stage"
      value               = "${var.stage}"
      propagate_at_launch = true
    },
    {
      key                 = "Name"
      value               = "${module.cluster_label.id}-edge-workers-ASG-Node"
      propagate_at_launch = true
    },
    {
      key                 = "kubernetes.io/cluster/${module.cluster_label.id}"
      value               = "owned"
      propagate_at_launch = true
    },
    {
      key                 = "k8s.io/cluster-autoscaler/enabled"
      value               = "true"
      propagate_at_launch = true
    },
  ]

  # Allowed values are Launch, Terminate, HealthCheck, ReplaceUnhealthy, 
  # AZRebalance, AlarmNotification, ScheduledActions, AddToLoadBalancer.
  suspended_processes = [
    "AZRebalance",
  ]

  depends_on = ["aws_eks_cluster.main", "aws_iam_role_policy_attachment.workers_EKSWorkerNodePolicy", "aws_iam_role_policy_attachment.workers_EKS_CNI_Policy"]
  lifecycle {
    ignore_changes = ["desired_capacity"]
  }
}

resource "aws_autoscaling_lifecycle_hook" "edge_workers" {
  name                   = "${module.cluster_label.id}-edge-workers-nodedrainerLCH"
  autoscaling_group_name = aws_autoscaling_group.edge_workers.name
  default_result         = "CONTINUE"
  heartbeat_timeout      = 300
  lifecycle_transition   = "autoscaling:EC2_INSTANCE_TERMINATING"

  notification_target_arn = aws_sqs_queue.instance_termination.arn
  role_arn                = aws_iam_role.autoscaling_instance_termination_notifications.arn
}
data "template_file" "system_workers_user_data" {
  template = file("./templates/workers_user_data.tpl.sh")

  vars {
    aws_region   = var.aws_region
    cluster_name = module.cluster_label.id

    node_labels      = "lifecycle=OnDemand,node-role.kubernetes.io/system=true"
    node_taints      = "system=true:NoSchedule"
    spot_node_labels = "lifecycle=Ec2Spot,node-role.kubernetes.io/spot-system=true"
    spot_node_taints = "spotInstance=true:PreferNoSchedule,system=true:NoSchedule"
    eviction_hard    = "memory.available<100Mi,nodefs.available<10%,nodefs.inodesFree<5%,imagefs.available<10%,imagefs.inodesFree<5%"
    kube_reserved    = "cpu=250m,memory=0.2Gi,ephemeral-storage=1Gi"
    system_reserved  = "cpu=250m,memory=0.2Gi,ephemeral-storage=1Gi"
  }
}

resource "aws_launch_template" "system_workers" {
  name          = "${module.cluster_label.id}-system-workers"
  image_id      = data.aws_ami.worker.id
  instance_type = "t3.small"

  credit_specification {
    # T3 instances are launched as unlimited by default. T2 instances are launched as standard by default.
    cpu_credits = "standard" # Can be "standard" or "unlimited"
  }

  network_interfaces {
    security_groups             = ["${aws_security_group.workers.id}"]
    device_index                = 0
    associate_public_ip_address = false
  }

  iam_instance_profile {
    name = aws_iam_instance_profile.system_workers.name
  }

  tag_specifications {
    resource_type = "instance"
    tags = (merge(module.cluster_label.tags, map(
      "Name", "${module.cluster_label.id}-system-worker-ASG-Node",
      "KubernetesCluster", "${module.cluster_label.id}",
      "kubernetes.io/cluster/${module.cluster_label.id}", "owned"
    )))
  }

  user_data = base64encode(data.template_file.system_workers_user_data.rendered)
  key_name  = data.terraform_remote_state.staging.ec2_key_name

  tags = module.cluster_label.tags
}

resource "aws_autoscaling_group" "system_workers" {
  name                = "${module.cluster_label.id}-system-workers"
  vpc_zone_identifier = ["${values(data.terraform_remote_state.staging.private_subnets_bohr)}"]
  min_size            = 1
  desired_capacity    = 1
  max_size            = 2

  mixed_instances_policy {
    instances_distribution {
      on_demand_allocation_strategy            = "prioritized"  # Valid values: prioritized. Default: prioritized
      spot_allocation_strategy                 = "lowest-price" # Valid values: lowest-price. Default: lowest-price.
      on_demand_base_capacity                  = 0
      on_demand_percentage_above_base_capacity = 0
      # EC2 Auto Scaling selects the cheapest Spot pools and evenly allocates Spot capacity across the number of Spot pools that you specify.
      spot_instance_pools = 2 # Default: 2
    }

    launch_template {
      launch_template_specification {
        launch_template_id = aws_launch_template.system_workers.id
        version            = "$$Latest"
      }
      override {
        instance_type = "t3.small"
      }
      override {
        instance_type = "t3a.small"
      }
      override {
        instance_type = "t2.small"
      }
    }
  }

  tags = [
    {
      key                 = "Namespace"
      value               = "${var.namespace}"
      propagate_at_launch = true
    },
    {
      key                 = "Stage"
      value               = "${var.stage}"
      propagate_at_launch = true
    },
    {
      key                 = "Name"
      value               = "${module.cluster_label.id}-system-workers-ASG-Node"
      propagate_at_launch = true
    },
    {
      key                 = "kubernetes.io/cluster/${module.cluster_label.id}"
      value               = "owned"
      propagate_at_launch = true
    },
  ]

  # Allowed values are Launch, Terminate, HealthCheck, ReplaceUnhealthy, 
  # AZRebalance, AlarmNotification, ScheduledActions, AddToLoadBalancer.
  suspended_processes = [
    "AZRebalance",
  ]

  depends_on = ["aws_eks_cluster.main", "aws_iam_role_policy_attachment.system_workers_EKSWorkerNodePolicy", "aws_iam_role_policy_attachment.system_workers_EKS_CNI_Policy"]
}

resource "aws_autoscaling_lifecycle_hook" "system_workers" {
  name                   = "${module.cluster_label.id}-system-workers-nodedrainerLCH"
  autoscaling_group_name = aws_autoscaling_group.system_workers.name
  default_result         = "CONTINUE"
  heartbeat_timeout      = 300
  lifecycle_transition   = "autoscaling:EC2_INSTANCE_TERMINATING"

  notification_target_arn = aws_sqs_queue.instance_termination.arn
  role_arn                = aws_iam_role.autoscaling_instance_termination_notifications.arn
}
